---
title: 'Azərbaycan dili üçün fine-tune edilmiş GPT-2 modeli'
date: '2023-02-14'
tags: []
draft: false
summary: 'GPT-2 modeli nədir və necə işləyir? Digər transformerlərdən fərqi nədir? Performansı necədir? azerbaijani-gpt2 modeli'
images: ['/static/blogs/gpt-2-az/gpt2.png']
authors: ['default']
---

# Azərbaycan dili üçün fine-tune edilmiş GPT-2 modeli

Bu məqalədə OpenAI şirkəti tərəfindən yaradılmış və NLP cəmiyyəti tərəfindən kifayət qədər marağa səbəb olmuş GPT ailəsinin ən çox danışılan üzvündən - GPT-2 modelindən bəhs edəcəm.
Məqalə aşağıdakı hissələrdən ibarətdir:

1. Dil modeli dedikdə nə başa düşək?
2. Dil modeli üçün transformerlər
4. GPT-2 necə işləyir?
5. "azerbaijani-gpt2 modeli"-nin train olunması və performansı

## Dil modeli dedikdə nə başa düşək?

Dil modelinə belə bir tərif versək, cümlənin əvvəlini nəzərə alıb növbəti sözü proqnozlaşdıra bilən maşın öyrənmə modeli deyə bilərik. Nümunə olaraq, mobil telefonlarımızda olan avtomatik yazma xidmətini göstərə bilərik.

![](https://1.bp.blogspot.com/-Oz-oMYfar8I/WSW90Jo866I/AAAAAAAAB1k/GO-9rpbpTcMhsfy3edD3lgcjXlLlTQjlwCLcB/s1600/image6.gif)

Bu mənada deyə bilərik ki, GPT-2 əsasən klaviatura tətbiqinin növbəti söz proqnozlaşdırma xüsusiyyətidir, lakin telefonunuzda olduğundan daha böyük və daha mürəkkəbdir. 
GPT-2, OpenAI tədqiqatçıları tərəfindən WebText adlı nəhəng 40GB verilənlər bazası üzərində train edilmişdir. Train edilmiş GPT-2-nin ən kiçik variantı bütün parametrlərini saxlamaq üçün 500MB yaddaş tutur. Ən böyük GPT-2 variantı isə bundan 13 dəfə böyükdür.
GPT-2 1,5 milyard parametrə malikdir. Müqayisə üçün deyim ki, bu GPT-1-dən 10 dəfə çoxdur. (117M parametr)

![](https://jalammar.github.io/images/gpt2/gpt2-sizes-hyperparameters-3.png)
Original transformer modeli encoder və dekoder adlanan bloklardan ibarətdir. Bu arxitektura uzun müddət uğurlu hesab edilirdi, əsasən də tərcümə üçün istifadə olunan modellərdə çox uyğun idi. Lakin GPT ailəsindən olan modellər, original transformer arxitekturasının encoder hissəsini tələb etmir, enkoder blokları mövcud deyil, başqa şəkildə desək, dekoder enkoderə ekvivalentdir.
Enkoder-dekoder arxitekturasından istifadə etdiyimiz hallar, adətən, bir mətnin digər mətnə çevrilməsi zamanı olur.(məsələn: azərbaycan dilindən ingilis dilinə tərcümə modeli)

GPT-2 modeli isə Vikipediya məqalələri kimi mətnlər üzərində öyrədilib. Buna görə də, bu hallarda, yalnız dekoderdən istifadə etməyə daha çox üstünlük verilir.

## "azerbaijani-gpt2 modeli"-nin train olunması və performansı

Azərbaycan dili üçün GPT modelinin fine tune edilməsi ümumi olaraq aşağıdakı şəkildə göstərilmişdir.

![](https://user-images.githubusercontent.com/31247506/218794844-dddeb35c-c1c4-4615-987b-e50ecb94bc0f.png)

* Azərbaycan dilində məqalələrinin və GPT-2 model, tokenizerlərini yükləyirik;

* Azərbaycan dili datası ilə GPT-2 tokenizerini train edirik;

* Azərbaycan dili datası ilə GPT-2 modelini fine-tune edirik;

İstifadə etdiyimiz wikipedia dataseti Azərbaycan dilində 130k-a yaxın məqalədən və təqribən 14M sözdən ibarətdir. Bu, GPT-2 modelinin üzərində train edildiyi WebText datasetindən 35 dəfəyə yaxın kiçikdir. Rəsmi məqalədə müəlliflər train dataset haqqında bunları qeyd ediblər:

>The resulting dataset, WebText, contains the text subset of these 45 million links. To extract the text from HTML responses we use a combination of the Dragnet (Peters &Lecocq, 2013) and Newspaper content extractors. All results presented in this paper use a preliminary version of WebText which does not include links created after Dec 2017 and which after de-duplication and some heuristic based cleaning contains slightly over 8 million documents for a total of 40 GB of text. We removed all Wikipedia documents from WebText since it is a common data source for other datasets and could complicate analysis due to overlapping training data with test evaluation tasks.

Azərbaycan dilində lüğətə malik GPT-2 tokenizer əldə etmək üçün bu addımı həyata keçirməliyik:

Transformers kitabxanasından (Hugging Face) əvvəlcədən hazırlanmış GPT-2 Tokenizer & Modelini (İngilis dili korpusu ilə hazırlanmış) yükləyirik. O, bizə lazım olan tokenizer strukturunu və əvvəlcədən öyrədilmiş model weightlərini verəcək. __GPT-2 modelini Azərbaycan train etmək üçün dilində GPT-2 modelimizi təsadüfi weightlərdən başlayaraq öyrətməkdənsə, başqa dildə öyrədilmiş weightlərdən başlayaraq öyrətmək daha məntiqlidir.__

Tokenize etmək üçün Byte-level BPE (BBPE) Tokenizer istifadə etmişəm.

## Nəticələr

## İstifadə qaydası
